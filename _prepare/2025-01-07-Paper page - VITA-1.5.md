## Paper page - VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction

Here is the summary of the contents in no more than 50% of the total contents length:

Hugging Face

 Papers
        
arxiv:2501.01957


VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction
Published on Jan 3

Authors:
Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He

Abstract
We propose a multi-stage training methodology that enables fluent vision and speech interaction. Our approach preserves strong vision-language capacity and enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules. We demonstrate our model's strong visual and speech capabilities across benchmarks for image, video, and speech tasks.

View arXiv page
View PDF

Community

akhaliq

Paper submitter

librarian-bot

6 days ago
This is an automated message from the Librarian Bot... 

Models citing this paper 0
Datasets citing this paper 0
Spaces citing this paper 0
Collections including this paper 14

#### Translation 

<document>
Hugging Face ( )

 Papers ( )


arxiv:2501.01957


VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction ( )
出版于 Jan 3

作者：
Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He

摘要
我们提出了多阶段训练方法，启用流畅的视觉和语音交互。我们的方法保留了强大的视觉语言能力，并且能够高效地实现语音对话能力，而不需要单独的ASR和TTS模块。我们展示了模型在图像、视频和语音任务上的强大视觉和语音能力。

查看 arXiv 页面
查看 PDF

社区

akhaliq ( )

论文投稿者

librarian-bot ( )

6天前
这是一个由 Librarian Bot 自动发送的消息...

引用此论文的模型 0 个
引用此论文的数据集 0 个
引用此论文的空间 0 个
包含此论文的集合 14 个</document>

#### Reference: 

https://huggingface.co/papers/2501.01957