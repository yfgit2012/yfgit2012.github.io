## John Schulman on scaling RL and research institutions

The following is a summary of the key points from the interview with Sam Altman (assumed to be the AI researcher mentioned in the document):

---

### **1. AI Development and Technological Trends**
- **Computing Power vs. Technical Methods**:  
  Sam believes that AI progress is not solely dependent on computing power; technical method optimization (e.g., post-training techniques, fine-tuning data construction) is equally critical. For example, the birth of ChatGPT may have occurred earlier than the current computing power level, indicating that technological breakthroughs require appropriate prerequisites.  
- **AGI Timeline Prediction**:  
  Researchers generally underestimate the time required to achieve AGI; predictions should be extended by 2-3 times (e.g., the case of autonomous driving). However, the self-accelerating effect of AI may shorten the timeline, leading to uncertainty in predictions.

---

### **2. Tinker Product and AI Fine-Tuning Innovation**
- **Tinker's Core Positioning**:  
  As a foundational fine-tuning API, Tinker provides small primitives (e.g., training and sampling tasks), allowing users to directly call them via Python scripts without handling the complexity of GPUs/distributed systems, thereby lowering the threshold for AI model fine-tuning.  
- **Future Plans**:  
  - Support multi-modal training (text, image, audio, video).  
  - Expand into a full-stack solution, enabling non-experts to rapidly build customized models.  
  - Lower the threshold for AI entrepreneurship, allowing entrepreneurs to focus on model innovation rather than底层 computing power issues.

---

### **3. Research and Work Methods**
- **AI Tools for Research**:  
  - Use tools like Cursor, Claude Code to program, generating code snippets or supplementing research content.  
  - Efficiency in literature retrieval and open-source library searches is significantly improved.  
- **Work Routine**:  
  - Prefers thinking in cafes, away from distractions, to record inspirations.  
  - During project execution, deep reading of code/documentation is required to guide others in research.

---

### **4. Observations on AI Research Trends**
- **Surge in Researchers but Stable Breakthroughs**:  
  - The rigor of experiments in 70-90s papers was lower, whereas today's standards are higher (e.g., multi-task benchmark tests).  
  - Scaling and engineering drive domain maturity, eliminating the need to write code from scratch, relying instead on existing tools and codebases.  
- **Shift in Skill Requirements**:  
  - Engineering skills have become more critical, while research taste has relatively declined (simple ideas can achieve significant results through scaling).

---

### **5. Views on the Future of AI**
- **Technical Methods Take Center Stage**:  
  Sam emphasizes that the future of AI depends more on optimizing technical methods and research taste rather than purely relying on computing power.  
- **Revival of Classic Concepts**:  
  Forgotten theories (e.g., early reinforcement learning methods) may regain attention due to new demands or technological maturity.

---

### **Summary**  
Sam's interview highlights the multidimensional development path of AI technology:  
- **Tool Innovation** (e.g., Tinker) lowers technical barriers, driving application implementation;  
- **Research Method Optimization** (e.g., experimental rigor, engineering) enhances domain maturity;  
- **Balancing Computing Power and Technology**, avoiding over-reliance on a single factor.  
His insights provide practical directions and thinking frameworks for AI researchers, developers, and entrepreneurs.

#### Translation 

以下是关于Sam Altman（假设为文档中提到的AI研究者）的访谈内容核心要点总结：

---

### **1. AI发展与技术趋势**
- **算力 vs 技术方法**：  
  Sam认为AI的进步并非单纯依赖算力，技术方法优化（如后训练技术、微调数据构建）同样关键。例如，ChatGPT的诞生可能早于当前算力水平，说明技术突破需要合适的前提条件。
- **AGI时间线预测**：  
  研究人员普遍低估了实现AGI的时间，需将预测时间延长2-3倍（如自动驾驶的案例）。但AI的自我加速效应可能缩短时间线，导致预测存在不确定性。

---

### **2. Tinker产品与AI微调创新**
- **Tinker的核心定位**：  
  作为底层微调API，Tinker提供小型原语（如训练和采样任务），用户可通过Python脚本直接调用，无需处理GPU/分布式系统复杂性，降低AI模型微调门槛。
- **未来规划**：  
  - 支持多模态训练（文本、图像、音频、视频）。
  - 扩展为全栈解决方案，使非专家用户也能快速构建定制模型。
  - 降低AI创业门槛，让创业者专注于模型创新而非底层算力问题。

---

### **3. 研究与工作方法**
- **AI工具辅助研究**：  
  - 使用Cursor、Claude Code等工具编程，通过AI生成代码片段或补充研究内容。
  - 文献检索和开源库查找效率大幅提升。
- **工作日常**：  
  - 喜欢在咖啡馆思考，远离干扰，记录灵感。
  - 项目执行阶段需深入阅读代码/文档，指导他人研究。

---

### **4. AI领域研究趋势观察**
- **研究者数量激增但突破速度稳定**：  
  - 70-90年代的论文实验严谨性较低，而如今标准更高（如多任务基准测试）。
  - 规模化和工程化推动领域成熟，无需从零编写代码，依赖现有工具和代码库。
- **技能需求变化**：  
  - 工程技能重要性上升，研究品味相对下降（简单想法通过规模化可取得显著成果）。

---

### **5. 对AI未来的看法**
- **技术方法回归**：  
  Sam更倾向于认为AI的未来取决于技术方法和研究品味的优化，而非单纯算力堆砌。
- **经典理念的复兴**：  
  被遗忘的理论（如强化学习早期方法）可能因新需求或技术成熟而重新被重视。

---

### **总结**
Sam的访谈强调了AI技术的多维发展路径：  
- **工具创新**（如Tinker）降低技术门槛，推动应用落地；  
- **研究方法优化**（如实验严谨性、工程化）提升领域成熟度；  
- **算力与技术的平衡**，避免过度依赖单一因素。  
他的观点为AI研究者、开发者和创业者提供了实践方向和思考框架。

#### Reference: 

https://www.youtube.com/watch?v=29BYxvvF1iM