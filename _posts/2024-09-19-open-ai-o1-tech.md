## OpenAI o1: post-training scaling law

Here is the translation of the contents into English:

1. **Reinforcement Learning and Implicit Thought Chain**: o1 uses reinforcement learning for training, introducing dynamic reasoning tokens to suggestively adopt "implicit thought chains" to think about problems. This enables the model to self-optimize and improve.
2. **Post-Training Scaling Rule**: Model releases mean AI capabilities are boosted beyond pre-training phases, allowing for enhanced exploration time through post-training scaling rules and increased inference time to boost performance.
3. **Self-Reflection and Bootstrap Capability**: The o1 model, based on self-reflection, not only boosts bootstrap capability but also significantly enhances its ability to resolve complex problems without prior knowledge, forming high-quality data flywheels, and advancing further towards super-intelligence.
4. **Reasoning Ability and Model Instruction Following Ability**: Although reasoning abilities have seen significant improvements in complex tasks like math and physics, progress has been limited in language generation tasks. OpenAI o1 excels at reasoning but cannot be relied upon as a proficient agent or assistant.
5. **Core Problem**: Balancing the relationship between these two aspects may become a core problem for future large model development.

In summary, this article provides an analysis of the technical advantages and potential limitations of the o1 model.

#### Translation 

这篇文章主要讨论了OpenAI o1模型的特点和技术优势。以下是总结：

1. **强化学习与隐式思维链**：o1使用强化学习进行训练，通过引入动态的推理Token启发式地采用“隐式思维链”来思考问题。这使得模型能够自我优化和改进。
2. **后训练缩放法则**：模型的发布意味着AI能力的提升，不再局限于预训练阶段，还可以通过在后训练阶段中提高强化学习训练的探索时间，以及增加模型推理的思考时间来提升模型的性能。
3. **自我反思和Bootstrap能力**：基于自我反思的o1模型不仅能够提升模型的Bootstrap能力，还将大大提升模型对于没有见过的复杂问题的解决能力，形成大量高质量的数据飞轮，从而向最终的超级智能迈进一步。
4. **推理能力和模型指令跟随能力**：尽管在数学、物理等复杂任务上的推理能力有了大幅的提升，但是在一些语言生成任务上并没有体现出更大的进步。OpenAI o1擅长于推理能力，但是并不能作为一个很好的Agent和助手。
5. **核心问题**：如何去平衡二者的关系可能会成为今后大模型发展的一个核心问题。

总之，这篇文章提供了对o1模型的技术优势和潜在局限性的分析。