## Hinton Nobel Prize speech

Here is the translation of the contents from the XML tags:

This lecture mainly discusses the development and progress of deep neural networks (DNNs), particularly the introduction of Boltzmann machines (BM) and restricted Boltzmann machines (RBM) and their applications in image recognition, etc.

Firstly, the speaker mentions that a Boltzmann machine is an early neural network model that simulates human brain activity to achieve learning and prediction functions. Boltzmann machines can well capture statistical rules in images and generate more realistic images. However, their learning speed is very slow, especially when the network size is large.

To solve this problem, a variant called restricted Boltzmann machine (RBM) was proposed. The main difference between RBM and ordinary BM lies in that its neural connections are limited, only allowing visible units and hidden units to be connected while hidden units do not connect with each other. This greatly simplifies the calculation process, making the learning speed significantly improved.

Then, the speaker mentions the shortcut learning algorithm, which is a method used to train RBMs. This algorithm calculates the probability of each hidden unit being activated and visible unit being activated to achieve learning. This algorithm can greatly reduce computational costs and quickly learn correct weights.

Later, the speaker talks about stacking multiple RBMs to build a deep neural network. By continuously training and stacking RBMs, it is possible to gradually learn more complex feature representations, achieving breakthroughs in image recognition, speech recognition, etc.

Finally, the speaker mentions that although these early learning algorithms are not perfect solutions, they have laid the foundation for later more advanced algorithms and technologies, and can avoid backpropagation's reverse paths, ultimately pointing us towards understanding how the human brain learns.

In short, this lecture is about deep neural networks and Boltzmann machines in image recognition, etc. and their progress.

#### Translation 

这篇演讲主要是关于深度神经网络（Deep Neural Network, DNN）的发展和进步。特别是关于玻尔兹曼机（Boltzmann Machine, BM）和受限玻尔兹曼机（Restricted Boltzmann Machine, RBM）的介绍及其在图像识别等领域的应用。

首先，演讲者提到玻尔兹曼机是一种早期的神经网络模型，它通过模拟人脑的大脑活动来实现学习和预测功能。玻尔兹曼机能够很好地捕捉图像中的统计规律，从而生成更逼真的图像。但是，它们的学习速度非常慢，尤其是在网络规模比较大的时候。

为了解决这个问题，一种叫做受限玻尔兹曼机（RBM）的变体被提出来。RBM 和普通玻尔兹曼机的主要区别在于，它的神经元连接是受限的，只允许可见单元和隐藏单元之间有连接，而隐藏单元之间没有连接。这大大简化了计算过程，使得学习速度得到了显著提高。

然后，演讲者提到捷径学习算法，这是一种用于训练RBM的方法。这个方法通过计算每个隐藏单元被激活的概率和可见单元被激活的概率来实现学习。这种算法能够大大减少计算成本，并且能够快速学习到正确的权重。

之后，演讲者提到了堆叠多个RBM来构建一个深度神经网络的方法。这通过不断地训练和堆叠RBMs，可以逐渐学习到更复杂的特征表示，从而实现图像识别、语音识别等领域的突破。

最后，演讲者提到这些早期的学习算法虽然不是最完美的解决方案，但它们为后来更先进的算法和技术的发展奠定了基础，并且能够避免反向传播的逆向路径，最终会为我们理解人脑如何学习指明道路。

总之，这篇演讲是关于深度神经网络和玻尔兹曼机在图像识别等领域的应用及其进步。

#### Reference: 

https://www.youtube.com/watch?v=lPIVl5eBPh8&t=0s