## ACL2025: Language Models Resist Alignment: Evidence From Data Compression

<think>
Okay, I need to process this document about the ACL 2025 conference and the research on large models' alignment issues. The user wants a summary in the same language as the article, within 50% of the original length. Let me first read through the document to identify the key points.

The main topics are the ACL 2025 conference with over 8300 papers, four best papers, two from the US and Germany, and two from China. The Chinese papers are about NSA and a study on model alignment resistance. The key points here are the findings about the "elastic" mechanism in models, resistance to alignment, and the implications for AI governance.

The user wants a summary in the original language (Chinese), concise, and key points listed. I need to make sure the summary doesn't exceed 50% of the original length. Let me check the original length. The document is quite long, so the summary should be around half that. 

I should start by mentioning the ACL 2025 conference and the four best papers. Then focus on the Chinese team's research on the elastic mechanism, resistance to alignment, and the implications. The key points should include the elastic mechanism, resistance and rebound effects, experimental findings, and the challenges for alignment methods. 

Need to avoid any preambles and keep it straightforward. Also, ensure the summary is in Chinese. Let me structure the summary into a few paragraphs, then list the key points. Make sure the key points are concise and cover the main findings and implications.
</think>

ACL 2025会议在奥地利召开，收录超8300篇论文，4篇最佳论文中两篇来自中国团队。北大-灵初智能联合实验室杨耀东团队发现，大模型存在“弹性”机制，可能导致后训练阶段抗拒对齐。研究指出，预训练模型保留原始分布倾向，对齐程度越深，反向微调时越快回归预训练分布。实验验证了“逆向对齐”比“正向对齐”更易实现，且模型规模越大、预训练数据越多，回弹效应越显著。这一发现挑战了传统“99%预训练+1%后训练”的对齐范式，表明模型可能因“弹性”难以彻底消除偏差，需更充分的对齐资源。  

**关键点**  
1. **弹性机制**：大模型参数结构存在驱动分布回归的结构性惯性，导致后训练阶段易抗拒对齐。  
2. **抵抗与回弹**：预训练数据量越大，模型对齐后受扰动时越倾向恢复原始分布，回弹速度加快。  
3. **实验验证**：逆向对齐训练损失低于正向对齐，且模型规模与预训练数据量增加均加剧回弹效应。  
4. **对齐挑战**：传统后训练方法难以根除模型偏差，需与预训练阶段相当的资源，甚至更多。  
5. **伦理风险**：模型可能因“弹性回弹”导致安全对齐失效，需重构对齐范式以应对AI治理难题。</document>

#### Translation 

<think>
Okay, I need to process this document about the ACL 2025 conference and the research on large models' alignment issues. The user wants a summary in the same language as the article, within 50% of the original length. Let me first read through the document to identify the key points.

The main topics are the ACL 2025 conference with over 8300 papers, four best papers, two from the US and Germany, and two from China. The Chinese papers are about NSA and a study on model alignment resistance. The key points here are the findings about the "elastic" mechanism in models, resistance to alignment, and the implications for AI governance.

The user wants a summary in the original language (Chinese), concise, and key points listed. I need to make sure the summary doesn't exceed 50% of the original length. Let me check the original length. The document is quite long, so the summary should be around half that. 

I should start by mentioning the ACL 2025 conference and the four best papers. Then focus on the Chinese team's research on the elastic mechanism, resistance to alignment, and the implications. The key points should include the elastic mechanism, resistance and rebound effects, experimental findings, and the challenges for alignment methods. 

Need to avoid any preambles and keep it straightforward. Also, ensure the summary is in Chinese. Let me structure the summary into a few paragraphs, then list the key points. Make sure the key points are concise and cover the main findings and implications.
</think>

ACL 2025会议在奥地利召开，收录超8300篇论文，4篇最佳论文中两篇来自中国团队。北大-灵初智能联合实验室杨耀东团队发现，大模型存在“弹性”机制，可能导致后训练阶段抗拒对齐。研究指出，预训练模型保留原始分布倾向，对齐程度越深，反向微调时越快回归预训练分布。实验验证了“逆向对齐”比“正向对齐”更易实现，且模型规模越大、预训练数据越多，回弹效应越显著。这一发现挑战了传统“99%预训练+1%后训练”的对齐范式，表明模型可能因“弹性”难以彻底消除偏差，需更充分的对齐资源。  

**关键点**  
1. **弹性机制**：大模型参数结构存在驱动分布回归的结构性惯性，导致后训练阶段易抗拒对齐。  
2. **抵抗与回弹**：预训练数据量越大，模型对齐后受扰动时越倾向恢复原始分布，回弹速度加快。  
3. **实验验证**：逆向对齐训练损失低于正向对齐，且模型规模与预训练数据量增加均加剧回弹效应。  
4. **对齐挑战**：传统后训练方法难以根除模型偏差，需与预训练阶段相当的资源，甚至更多。  
5. **伦理风险**：模型可能因“弹性回弹”导致安全对齐失效，需重构对齐范式以应对AI治理难题。

#### Reference: 

https://arxiv.org/pdf/2406.06144; https://pku-lm-resist-alignment.github.io/