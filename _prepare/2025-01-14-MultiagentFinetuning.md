## Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains

Here is a summary of the contents in no more than 50% of the total contents length:

Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains

* A set of language models are initialized from the same base model and then specialized by independently updating each model using data generated by the model under multiagent interaction with other models.
* By training each model on independent sets of data, this approach enables specialization across models and diversification over the set of models.
* The overall system is able to autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods.

Method

* Multiagent Finetuning constructs a multiagent set of language models over multiple rounds of finetuning, where each model specializes to become a generation or critic agent based on previous generations.
* A set of generation and critic agents are finetuned using data generated by the models under multiagent interaction.

Results

* The approach consistently improves performance across multiple iterations of finetuning, while single-agent self-improvement methods lead to a plateau of performance.
* Multiagent finetuning outperforms existing self-improvement methods for 4 different LLM models.
* The method preserves diversity over responses and generalizes to new datasets.

#### Translation 

<document>Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains

* 多个语言模型从相同的基准模型中初始化，然后通过独立更新每个模型，使用由模型生成的数据，在多代理互动下专门用于其他模型。
* 通过在每个模型上训练独立的数据集，使这种方法能够跨模型实现专业化，并在模型集合中实现多样性。
* 整个系统可以通过反复对模型进行微调，超过单一代理自我改进方法来自动改进。

Method

* 多代理微调构建多代理语言模型集合，以便在多轮微调中，每个模型专门成为生成或评论代理，基于之前的几代。
* 使用多代理互动下模型生成的数据，对于生成和评论代理进行微调。

Results

* 在多次微调迭代中，该方法始终改进性能，而单一代理自我改进方法导致性能平衡。
* 多代理微调比现有的自我改进方法更好，针对4种不同的LLM模型。
* 方法保留了响应的多样性，并推广到新数据集。</document>

#### Reference: 

https://llm-multiagent-ft.github.io/