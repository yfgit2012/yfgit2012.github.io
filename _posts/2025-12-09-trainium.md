## AWS Trainium3 Deep Dive

### **Deep Analysis of AWS Trainium3 Chip**  
#### **1. Design and Architecture**  
- **Balance Between Liquid Cooling and Air Cooling**:  
  - **NL72x2 Switched** (high-density rack) adopts liquid cooling design, but AWS prefers **air cooling** due to higher cost efficiency. Wind-cooled data centers maintain a stable **PUE (Power Usage Effectiveness)** of **around 1.2**, and have better compatibility, supporting mixed deployment of CPUs, storage, and other devices.  
  - **Standardized Design**: AWS has maintained air cooling dominance in data centers since 2021, using liquid cooling only locally in high-density clusters to avoid deployment complexity caused by changes in cooling methods.  

- **Hardware Architecture**:  
  - **Cable-less Architecture**: Reduces cabling costs and improves deployment efficiency.  
  - **Multi-vendor Strategy**: Collaborates with companies like Astera Labs and Credo, reducing hardware costs through supply chain rebates (e.g., Credo AEC cable rebates reach up to 325.6%).  
  - **LNC Support**: Early versions only support LNC=1 or 2 (corresponding to HBM capacity of 36GB/72GB), and LNC=8 requires complex compiler optimizations, with support expected in 2026.  

---

#### **2. Performance and Metrics**  
- **Computing Power**:  
  - **MXFP8 Computing Power**: Sufficient to support training of large models with trillion parameters.  
  - **Memory Bandwidth**: Supports efficient data processing, reducing memory bottlenecks.  
  - **Cluster Scalability**: Achieves scalability for tens of thousands of chips via Switched architecture.  

- **Advantages of Liquid-cooled Racks**:  
  - **NL72x2 Switched**: Provides 144GB HBM per rack, supporting high-density computing, suitable for top-tier customers (e.g., Anthropic).  

---

#### **3. Software Ecosystem and Open-source Strategy**  
- **Native PyTorch Support**:  
  - **Open-source PyTorch Backend**: Compatible with native APIs and eager execution mode, allowing developers to avoid migrating CUDA code, significantly reducing learning costs.  
  - **Torch Compile Support**: Converts PyTorch FX graphs into Trainium instruction sets via a custom backend, enabling automatic optimization (currently supports SimpleFSDP).  

- **Open-source Plan**:  
  - **Phase 1**: Open-source core components like NKI compiler, matrix multiplication library, and communication library.  
  - **Phase 2**: Open-source XLA graph compiler and JAX software stack, competing with NVIDIA’s CUDA ecosystem.  

- **Toolchain**:  
  - **Neuron Explorer**: A performance analysis tool that supports visualizing engine utilization, HBM bandwidth, DMA transfers, and automatically identifies bottlenecks to provide optimization suggestions.  

---

#### **4. Cost and Deployment Advantages**  
- **Hardware Costs**:  
  - **Air Cooling Design**: Reduces data center construction costs, offering better cost efficiency than liquid cooling GB200.  
  - **Supply Chain Rebates**: AWS secures supplier stock options (e.g., Credo rebates reach 325.6%) through procurement agreements, further compressing hardware costs.  

- **Deployment Flexibility**:  
  - **Compatibility**: Air cooling supports mixed deployment of multiple devices, avoiding device isolation issues in liquid cooling data centers.  

---

#### **5. Comparison with Competitors**  
| **Competitor**       | **Advantages**                                                                 | **Disadvantages**                                                                 |
|----------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **NVIDIA GB200** | Single-chip FP8 computing power reaches 327TFLOPs, supports NVLink 448G BiDi protocol, strong cluster scalability       | High single-card cost (tens of thousands of dollars), high cost of liquid cooling data centers, poor TCO performance |
| **AMD MI450X**  | Liquid cooling design, supports UALink protocol, TCO performance close to Trainium3                        | Lagging software ecosystem (poor MoE support, PyTorch compatibility), late market entry, missed window period   |
| **Google TPUv7**   | Mature software ecosystem, JAX framework support, TCO performance comparable to Trainium3                    | Limited to Google Cloud TPU services, limited external sales, poorer compatibility than PyTorch native support         |
| **Trainium3**   | Balanced performance, cost, and ecosystem, native PyTorch support, air cooling reduces deployment costs             | Requires time to mature software ecosystem, LNC=8 support lags, 4-bit precision optimization pending                 |

---

#### **6. Summary and Outlook**  
- **Core Advantages**:  
  - **Performance and Cost Balance**: MXFP8 computing power + air cooling design reduces TCO.  
  - **Open-source Ecosystem**: Native PyTorch support + open-source strategy attracts developer communities.  
  - **Supply Chain Strategy**: Reduces hardware costs via rebates, enhancing competitiveness.  

- **Challenges and Future**:  
  - **Software Ecosystem**: Needs 1-2 years to improve MoE support, LNC=8 optimization, etc.  
  - **Market Adoption**: General developers need to adapt to LNC limitations, but top-tier customers (e.g., Anthropic) can directly manage multiple NeuronCores.  

- **Industry Impact**:  
  - **Four-way Competition**: Trainium3 may join NVIDIA, AMD, and Google in dominating the AI accelerator market, driving industry technological and ecosystem development.  

**Final Conclusion**: Trainium3, with its balanced performance, cost, and ecosystem, is expected to become a significant force in the AI accelerator market, but its success still depends on time to validate the maturity of its software ecosystem and market acceptance.

#### Translation 

### **AWS Trainium3 芯片深度解析**  
#### **1. 设计与架构**  
- **液冷与风冷的平衡**：  
  - **NL72x2 Switched**（高密度机架）采用液冷设计，但AWS更倾向于**风冷**，因其成本效率更高。风冷数据中心的PUE（电源使用效率）稳定在**1.2左右**，且兼容性更强，支持CPU、存储等多种设备混布。  
  - **标准化设计**：AWS数据中心自2021年以来保持风冷主导，仅在高密度集群中局部采用液冷，避免因冷却方式改变导致的部署复杂性。  

- **硬件架构**：  
  - **无电缆架构**：减少布线成本，提升部署效率。  
  - **多供应商策略**：与Astera Labs、Credo等公司合作，通过供应链返利降低硬件成本（如Credo AEC电缆返利高达325.6%）。  
  - **LNC支持**：早期版本仅支持LNC=1或2（对应HBM容量36GB/72GB），LNC=8需复杂编译器优化，预计2026年支持。  

---

#### **2. 性能与指标**  
- **计算能力**：  
  - **MXFP8算力**：足以支撑万亿参数级大模型训练。  
  - **内存带宽**：支持高效数据处理，降低内存瓶颈。  
  - **集群扩展**：通过Switched架构实现数万个芯片的扩展能力。  

- **液冷机架优势**：  
  - **NL72x2 Switched**：每机架提供144GB HBM，支持高密度计算，适合顶级客户（如Anthropic）。  

---

#### **3. 软件生态与开源策略**  
- **PyTorch原生支持**：  
  - **开源PyTorch后端**：兼容原生API和eager执行模式，开发者无需迁移CUDA代码，学习成本大幅降低。  
  - **Torch Compile支持**：通过自定义后端将PyTorch FX图转换为Trainium指令集，实现自动优化（目前支持SimpleFSDP）。  

- **开源计划**：  
  - **第一阶段**：开源NKI编译器、矩阵乘法库、通信库等核心组件。  
  - **第二阶段**：开源XLA图编译器和JAX软件栈，对标英伟达CUDA生态。  

- **工具链**：  
  - **Neuron Explorer**：性能分析工具，支持可视化引擎利用率、HBM带宽、DMA传输等指标，自动识别瓶颈并提供优化建议。  

---

#### **4. 成本与部署优势**  
- **硬件成本**：  
  - **风冷设计**：降低数据中心建设成本，相比液冷GB200更具成本效益。  
  - **供应链返利**：AWS通过采购协议获得供应商股票期权（如Credo返利达325.6%），进一步压缩硬件成本。  

- **部署灵活性**：  
  - **兼容性**：风冷设计支持多种设备混布，避免液冷数据中心的设备隔离问题。  

---

#### **5. 与竞品的对比**  
| **竞品**       | **优势**                                                                 | **劣势**                                                                 |
|----------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **英伟达GB200** | 单芯片FP8算力达327TFLOPs，支持NVLink 448G BiDi协议，集群扩展性强       | 单卡成本高（数万美元），液冷数据中心建设成本高昂，每TCO性能不占优       |
| **AMD MI450X**  | 液冷设计，支持UALink协议，每TCO性能接近Trainium3                        | 软件生态落后（MoE支持、PyTorch适配不足），上市时间晚，错失市场窗口期   |
| **谷歌TPUv7**   | 软件生态成熟，JAX框架支持，每TCO性能与Trainium3相当                    | 仅限谷歌Cloud TPU服务，对外销售有限，兼容性不如PyTorch原生支持         |
| **Trainium3**   | 性能、成本、生态平衡，PyTorch原生支持，风冷设计降低部署成本             | 软件生态成熟度需时间，LNC=8支持滞后，4位精度优化待完善                 |

---

#### **6. 总结与展望**  
- **核心优势**：  
  - **性能与成本平衡**：MXFP8算力+风冷设计，降低TCO。  
  - **开源生态**：PyTorch原生支持+开源策略，吸引开发者社区。  
  - **供应链策略**：通过返利降低硬件成本，增强竞争力。  

- **挑战与未来**：  
  - **软件生态**：需1-2年完善MoE支持、LNC=8优化等。  
  - **市场普及**：普通开发者需适应LNC限制，但顶级客户（如Anthropic）可直接管理多NeuronCore。  

- **行业影响**：  
  - **四强争霸格局**：Trainium3或与英伟达、AMD、谷歌共同主导AI加速器市场，推动行业技术与生态发展。  

**最终结论**：Trainium3凭借性能、成本与生态的平衡，有望成为AI加速器市场的重要力量，但其成功仍需时间验证软件生态的成熟度与市场接受度。

#### Reference: 

https://newsletter.semianalysis.com/p/aws-trainium3-deep-dive-a-potential